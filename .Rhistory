* more predictor (independent) variables: multiple / multivar.
mtcars
penguin
penguins
knitr::opts_chunk$set(echo = TRUE)
ln(mpg ~ am)
lm(mpg ~ am)
lm(mtcars::mpg ~ am)
lm(mtcars::mpg ~ am)
data(mtcars)
data(mtcars)
lm(mpg ~ am)
data(mtcars)
lm(mpg ~ am, data=mtcars)
plot(mpg~am)
plot(mpg~am, data=mtcars)
lm(mpg ~ am, data=mtcars)
plot(mpg~cyl, data=mtcars)
mtcars
plot(mpg~hp, data=mtcars)
knitr::opts_chunk$set(echo = FALSE)
reg <- lm(mpg~hp, data=mtcars)
plot(mpg~hp, data=mtcars)
abline(reg)
knitr::opts_chunk$set(echo = FALSE)
stats::lm(mpg~hp, data=mtcars)
stats::summary.lm(reg)
stats::summary.lm(reg)
stats::summary(reg)
summary(reg)
summary(reg)
1.788e-07
knitr::opts_chunk$set(echo = FALSE, options(scipen = 100))
summary(reg)
0.0000001788<0.01
mtcars_ouput <-
mtcars %>%
dplyr::filter(!is.na(mpg) | !is.na(hp)) %>%
mutate(
model_stdres = reg %>% stats::rstandard(),
model_cook_dist = reg %>% stats::cooks.distance()
)
install.packages('tidyverse')
---
title: "Linear Regression"
author: "Nicholas Lambert"
date: "6/22/2021"
output:
pdf_document:
latex_engine: xelatex
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, options(scipen = 100))
#install.packages('tidyverse') # for analysis workflow
library(tidyverse)
```
# Simple regression
Regression analysis is a supervised machine learning approach
Special case of the general linear model
$$
\begin{aligned}
outcome_i &= (model) + error_i
\end{aligned}
$$
Predict (estimate) value of one outcome (dependent or target) variable as:
* one predictor or feature (independent) variable: simple / univariate
$$
\begin{aligned}
Y_i &= (b_0+b_1∗X_{i1})+ϵ_i
\end{aligned}
$$
* more predictors or features (independent) variables: multiple / multivar.
$$
\begin{aligned}
Y_i &= (b_0+b_1∗X_{i1}+b_2∗X_{i2}+⋯+b_M∗X_{iM})+ϵ_i
\end{aligned}
$$
## Example
Can we predict a a car’s miles per gallon (mpg) from horsepower?
body massi=(b0+b1∗flipper lengthi)+ϵi
$$
\begin{aligned}
mpg_i &= (b_0+b_1∗horsepower)+ϵ_i
\end{aligned}
$$
```{r}
reg <- lm(mpg~hp, data=mtcars)
plot(mpg~hp, data=mtcars)
abline(reg)
```
## Least squares
Least squares is the most commonly used approach to generate a regression model
The model fits a line:
* to minimise the squared values of the residuals (errors)
* that is squared difference between
* observed values
$$
\begin{aligned}
residual_i=observed_i−model_i \\\\
deviation=∑_i(observed_i−model_i)^2
\end{aligned}
$$
## Assumptions
* **Linearity**
* the relationship is actually linear
* **Normality of residuals**
* standard residuals are normally distributed with mean 0
* **Homoscedasticity of residuals**
* at each level of the predictor variable(s) the variance of the standard residuals should be the same (homo-scedasticity) rather than different (hetero-scedasticity)
* **Independence of residuals**
* adjacent standard residuals are not correlated
```{r}
summary(reg)
```
## Overall fit
The output indicates:
* p-value:  0.0000001788: p <.01 the model is significant
* derived by comparing F-statistic (45.46) to F distribution having specified degrees of freedom (1, 30)
* Report as: F(1, 30) = 45.46
* Adjusted R-squared: 0.5892:
* hoursepower can account for 58.92% variation in miles per gallon
* Coefficients
* Intercept estimate 30.09886 is significant
* horsepower (slope) estimate -0.06823 is significant
## Outliers and influential cases
```{r}
mtcars_ouput <-
mtcars %>%
dplyr::filter(!is.na(mpg) | !is.na(hp)) %>%
mutate(
model_stdres = reg %>% stats::rstandard(),
model_cook_dist = reg %>% stats::cooks.distance()
)
mtcars_ouput %>%
dplyr::select(mpg, model_stdres, model_cook_dist) %>%
dplyr::filter(abs(model_stdres) > 2.58 | model_cook_dist > 1)
```
knitr::opts_chunk$set(echo = FALSE, options(scipen = 100))
#install.packages('tidyverse') # for analysis workflow
library(tidyverse)
mtcars_ouput <-
mtcars %>%
dplyr::filter(!is.na(mpg) | !is.na(hp)) %>%
mutate(
model_stdres = reg %>% stats::rstandard(),
model_cook_dist = reg %>% stats::cooks.distance()
)
mtcars_ouput
View(mtcars_ouput)
mtcars_ouput %>%
dplyr::select(mpg, model_stdres, model_cook_dist) %>%
dplyr::filter(abs(model_stdres) > 2.58 | model_cook_dist > 1)
mtcars_ouput %>%
dplyr::select(mpg, model_stdres, model_cook_dist) %>%
dplyr::filter(abs(model_stdres) > 2.58 | model_cook_dist > 1)
?rstandard
?cooks.distance
mtcars_output %$%
stats::shapiro.test(
model_stdres
)
mtcars_output %>%
stats::shapiro.test(
model_stdres
)
mtcars_ouput
mtcars_output %>%
stats::shapiro.test(
model_stdres
)
mtcars_output <-
mtcars %>%
dplyr::filter(!is.na(mpg) | !is.na(hp)) %>%
mutate(
model_stdres = reg %>% stats::rstandard(), # provides leave-one-out cross validation residuals
model_cook_dist = reg %>% stats::cooks.distance() # provides estimate of the influence of a data point when performing a least-squares regression analysis
)
mtcars_output %>%
dplyr::select(mpg, model_stdres, model_cook_dist) %>%
dplyr::filter(abs(model_stdres) > 2.58 | model_cook_dist > 1)
mtcars_output %>%
stats::shapiro.test(
model_stdres
)
mtcars_output <-
mtcars %>%
dplyr::filter(!is.na(mpg) | !is.na(hp)) %>%
mutate(
model_stdres = reg %>% stats::rstandard(), # provides leave-one-out cross validation residuals
model_cook_dist = reg %>% stats::cooks.distance() # provides estimate of the influence of a data point when performing a least-squares regression analysis
)
mtcars_output %>%
dplyr::select(mpg, model_stdres, model_cook_dist) %>%
dplyr::filter(abs(model_stdres) > 2.58 | model_cook_dist > 1)
mtcars_output %>%
stats::shapiro.test(
model_stdres
)
mtcars_output
View(mtcars_output)
mtcars_output %>%
stats::shapiro.test(
model_stdres()
)
?shapiro.test
mtcars_output %>%
stats::shapiro.test(
model_stdres
)
stats::shapiro.test(
mtcars_output$model_stdres
)
mtcars_output %>%
stats::shapiro.test(
.$model_stdres
)
stats::shapiro.test(
mtcars_output.$model_stdres
)
stats::shapiro.test(
mtcars_output$model_stdres
)
plot(mtcars_output$model_stdres)
hist(mtcars_output$model_stdres)
qqnorm(mtcars_output$model_stdres)
knitr::opts_chunk$set(echo = FALSE, options(scipen = 100))
#install.packages('tidyverse') # for analysis workflow
library(tidyverse)
mtcars_output %>%
lmtest::bptest()
install.packages('lmtest')
library(lmtest) #  A collection of tests for diagnostic checking in linear regression models
mtcars_output %>%
lmtest::bptest()
lmtest::bptest(mtcars_output$model_stdres)
mtcars_output %>%
lmtest::bptest(model_stdres)
?bptest
lmtest::bptest(reg)
reg %>%
lmtest::bptest()
reg %>%
lmtest::dwtest()
reg %>%
lmtest::dwtest()
knitr::opts_chunk$set(echo = FALSE, options(scipen = 100))
#install.packages(c('tidyverse', 'lmtest'))
library(tidyverse) # for analysis workflow
library(lmtest) #  A collection of tests for diagnostic checking in linear regression models
reg %>%
car::vif()
reg %>%
vif()
reg %>%
car::vif() # multicollinearity can assessed by computing a score
library(car) # companion to Applied Regression
reg %>%
car::vif() # multicollinearity can assessed by computing a score
car::vif(reg) # multicollinearity can assessed by computing a score
?vif
reg <- lm(mpg~hp, data=mtcars)
plot(mpg~hp, data=mtcars)
abline(reg)
reg %>%
car::vif() # multicollinearity can assessed by computing a score
reg %>%
car::vif() # multicollinearity can assessed by computing a score
reg <- lm(mpg~log(hp), data=mtcars)
reg <- lm(mpg~log(hp), data=mtcars) %>% summary()
summary(reg)
reg <- lm(mpg~log(hp), data=mtcars)
summary(reg)
